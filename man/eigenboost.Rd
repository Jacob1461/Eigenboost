% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/eigenboost.R
\name{eigenboost}
\alias{eigenboost}
\title{Adaboost on reflected datasets}
\usage{
eigenboost(
  X,
  y,
  n_rounds = 10,
  reflection_every = 3,
  verbose = FALSE,
  control = NULL,
  reflections_type = "eigen",
  tree_depth = 1
)
}
\arguments{
\item{X}{A dataframe or matrix of predictors values.}

\item{y}{A response vector of 1 or -1.}

\item{n_rounds}{The number of trees in the ensemble.}

\item{reflection_every}{Perform a Dataset reflection every this many rounds.}

\item{verbose}{Print additional output.}

\item{control}{Additional settings for the CART trees}

\item{reflections_type}{The type of Dataset reflection to perform. Options include:
identity, for no data reflections (identical to normal Adaboost),
eigen, for eigenvector based reflections based on the dominant eigenvector of the class which reduces training error the most,
random, reflect the dataset in a random direction,
eigen-random, choose either eigen or random reflections with equal probability.}

\item{tree_depth}{The max depth of the CART trees.}
}
\value{
Returns a eigenboost object containing the alpha values for each tree,
each tree object, the householder matrix corresponding to that tree.
}
\description{
An implementation of adaboost using eigenvector based dataset reflections
using PCA and householder matrices.
}
\note{
Adaboost code adapted from JOUSboost implementation.
}
\examples{
\dontrun{
  set.seed(123)

# Make iris into a binary class classification dataset
iris2 <- iris[iris$Species != "setosa", ]
rownames(iris2) <- NULL
iris2$Species <- ifelse(iris2$Species == "versicolor", 1, -1)

# Make training and testing splits
train_indx <- sample(nrow(iris2), 65)
X_train <- iris2[train_indx, 1:4]
y_train <- as.numeric(iris2$Species[train_indx])
X_test <- iris2[-train_indx, 1:4]
y_test <- as.numeric(iris2$Species[-train_indx])

# Fit model
model <- eigenboost(X = X_train, y = y_train, reflection_every = 3, tree_depth = 1)

# Make predictions
preds <- predict.eigenboost(model, X_test)

# Evaluate
accuracy <- mean(preds == y_test)
print(accuracy)
}
}
\references{
Freund, Y. and Schapire, R. (1997). A decision-theoretic
generalization of online learning and an application to boosting, Journal of
 Computer and System Sciences 55: 119-139.
}
